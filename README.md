Gradient Descent Optimizers

I have implemented some gradient descent optimizers for linear regression : 

- Vanilla gradient descent
- Batch gradient Descent (with momentum)
- Adagrad
- RMSPROP
- Adam

TODO :

- Add level curve plot
- Nesterov acceleration
- KSGD
- More generic implementation (polynomial)
- Evolution of error on the fly
- Newton method (logistic regression, but maybe in a new repo)

